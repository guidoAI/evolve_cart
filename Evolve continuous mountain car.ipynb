{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolutionary Robotics\n",
    "\n",
    "In this notebook, we will discuss evolutionary robotics and have you implement an evolutionary algorithm to solve one of the tasks in the AI gym, namely the continuous mountain car task. As you will see, an initial implementation of an evolutionary algorithm for solving a robotics task is easily made. However, obtaining good results may - depending on the task - be hard, and understanding the solution may even be harder still.\n",
    "\n",
    "The figure below shows the typical evolutionary robotics approach. An initial population is randomly generated. Then there is an iterative process of: (1) Evaluating all individuals (genomes) in the population, resulting in a fitness value for each individual, (2) Selecting the individuals that will be allowed to procreate, i.e., form the new generation, and (3) Vary on the genomes of the selected individuals (using cross-over, mutation, etc.). The process typically terminates either after a specified number of generations, or after convergence to an optimal solution. Evaluation involves the conversion of the genome (genotype) to the phenotype (e.g., setting the weights of a neural network to the values in the genome). Then the phenotype is tested out on the task, typically in simulation but in some works also on real robots. In robotics tasks, evaluation is a stochastic process and execution of the task by the robot can take a long time. \n",
    "\n",
    "<img src=\"evolutionary_robotics_process.jpg\" width=\"50%\"></img>\n",
    "*Figure 1:* Depiction of the typical evolutionary robotics approach. Figure from: _Doncieux, S., Bredeche, N., Mouret, J. B., & Eiben, A. E. G. (2015). Evolutionary robotics: what, why, and where to. Frontiers in Robotics and AI, 2, 4._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MountainCarContinuous-v0\n",
    "\n",
    "In this notebook, you will apply an evolutionary robotics approach to the continuous mountain car task. In this task, the car needs to reach the flag on the right mountain, while observing its position and velocity, and acting by means of accelerating the car left or right. The car cannot go straight up the mountain, but has to build up momentum to succeed. The fitness function rewards reaching the flag, and punishes the use of control actions (using less energy to reach the hill top is better). Please see the general description of the task <A HREF=\"https://gym.openai.com/envs/MountainCarContinuous-v0/\" TARGET=\"_blank\">here</A> and the details of the task <A HREF=\"https://github.com/openai/gym/wiki/MountainCarContinuous-v0\" TARGET=\"_blank\">here</A>. Most importantly, it has two observations, i.e., the position and velocity along the trajectory, and one control input, i.e., the applied force, which has to be in the interval $[-1, 1]$.\n",
    "\n",
    "<img src=\"continuous_mountain_car.png\" width=\"50%\"></img>\n",
    "*Figure 2:* Screenshot of the continuous mountain car task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find code to evaluate an agent a single time in the mountain car environment. Please study the code, and note that the method ```act``` should be replaced in the end with a learned controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_cart\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class random_agent(object):\n",
    "    \"\"\"Random agent\"\"\"\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return [2.0*np.random.rand()-1.0]\n",
    "\n",
    "agent = random_agent()\n",
    "reward = run_cart.run_cart_continuous(agent, graphics=False)\n",
    "print('Reward = ' + str(reward))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a continuous time recurrent neural network as controller\n",
    "\n",
    "Of course, random control is not going to solve the task. Below, we introduce an agent that uses a continuous time recurrent neural network for control. This network was introduced by Randall Beer and used for one of the first times in the following article: Beer, Randall D. The dynamics of active categorical perception in an evolved model agent. Adaptive Behavior 11.4 (2003): 209-243.\n",
    "\n",
    "CTRNNs are different from typical artificial neural networks (ANNs) in that they have an activation state $s$ which leads to a 'neural inertia'. Specifically, the activation formula, expressed as a differential equation, is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tau_i \\dot{s_i} = -s_i + I_i + \\sum_{j=1}^{N}{w_{j,i} \\sigma (g_j(s_j + \\theta_j))},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\tau_i$ is the time constant of neuron $i$, $s_i$ its activation state, and $I_i$ its external inputs. $\\sigma$ is the activation function, $g_j$ the gain of neuron $j$, and $\\theta_j$ its bias. \n",
    "\n",
    "Below, we use <A HREF=\"https://github.com/madvn/CTRNN\" TARGET=\"_blank\">this CTRNN package</A>. It makes no explicit difference between types of neurons. So a network of size 10 means essentially a fully connected network of 10 neurons, where each neuron has connections to all other neurons and itself. In the code below, we give external inputs to the first two neurons, setting these inputs to the observations in the continuous mountain car task. We read out the last neuron as the output to be used for the car control. The activation function is $\\sigma(s) = \\frac{1}{1+e^{-s}}$.\n",
    "\n",
    "<FONT COLOR=\"red\">Exercise 1.</FONT>\n",
    "1. Run the network multiple times. What values does it output over the different runs? Why?\n",
    "2. Suppose that we want to start optimizing the weights and other parameters to have the car achieve the task. The ```act``` function below then has a fundamental problem. Can you spot what the problem is?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CTRNN import CTRNN\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class CTRNN_agent(object):\n",
    "    \n",
    "    \"\"\" Continuous Time Recurrent Neural Network agent. \"\"\"\n",
    "    \n",
    "    n_observations = 2;\n",
    "    n_actions = 1;\n",
    "    \n",
    "    def __init__(self, network_size, weights=[], taus = [], gains = [], biases = []):\n",
    "        self.network_size = network_size;\n",
    "        if(self.network_size < self.n_observations + self.n_actions):\n",
    "            self.network_size = self.n_observations + self.n_actions;\n",
    "        self.cns = CTRNN(self.network_size, step_size=0.1) \n",
    "        if(len(weights) > 0):\n",
    "            # weights must be a matrix size: network_size x network_size\n",
    "            self.cns.weights = csr_matrix(weights)\n",
    "        if(len(biases) > 0):\n",
    "            self.cns.biases = biases\n",
    "        if(len(taus) > 0):\n",
    "            self.cns.taus = taus\n",
    "        if(len(gains) > 0):\n",
    "            self.gains = gains\n",
    "    \n",
    "    def act(self, observation, reward, done):\n",
    "        external_inputs = np.asarray([0.0]*self.network_size)\n",
    "        external_inputs[0:self.n_observations] = observation\n",
    "        self.cns.euler_step(external_inputs)\n",
    "        return self.cns.outputs[-self.n_actions:]\n",
    "\n",
    "# set up a CTRNN agent:\n",
    "n_neurons = 10;\n",
    "weights = np.zeros([n_neurons, n_neurons])\n",
    "taus = np.asarray([0.1]*n_neurons)\n",
    "gains = np.ones([n_neurons,])\n",
    "biases = np.zeros([n_neurons,])\n",
    "agent = CTRNN_agent(n_neurons, weights=weights, taus = taus, gains = gains, biases = biases)\n",
    "\n",
    "# run the agent:\n",
    "reward = run_cart.run_cart_continuous(agent, simulation_seed=0, graphics=False)\n",
    "\n",
    "# print the reward:\n",
    "print('Reward = ' + str(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simplest Evolutionary Algorithm for evolving a CTRNN\n",
    "\n",
    "The code below shows a very simple evolutionary algorithm that optimizes all CTRNN parameters for the mountain car task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# added unpacking of genome:\n",
    "class CTRNN_agent(object):\n",
    "    \n",
    "    \"\"\" Continuous Time Recurrent Neural Network agent. \"\"\"\n",
    "    \n",
    "    n_observations = 2;\n",
    "    n_actions = 1;\n",
    "    \n",
    "    def __init__(self, network_size, genome = [], weights=[], taus = [], gains = [], biases = []):\n",
    "        \n",
    "        self.network_size = network_size;\n",
    "        if(self.network_size < self.n_observations + self.n_actions):\n",
    "            self.network_size = self.n_observations + self.n_actions;\n",
    "        self.cns = CTRNN(self.network_size, step_size=0.1) \n",
    "        \n",
    "        if(len(genome) == self.network_size*self.network_size+3*self.network_size):\n",
    "            # Get the network parameters from the genome:\n",
    "            weight_range = 3\n",
    "            ind = self.network_size*self.network_size\n",
    "            w = weight_range * (2.0 * (genome[:ind] - 0.5))\n",
    "            weights = np.reshape(w, [self.network_size, self.network_size])\n",
    "            biases = weight_range * (2.0 * (genome[ind:ind+self.network_size] - 0.5))\n",
    "            ind += self.network_size\n",
    "            taus = genome[ind:ind+self.network_size] + 1e-5\n",
    "            ind += self.network_size\n",
    "            gains = 2.0 * (genome[ind:ind+self.network_size]-0.5)\n",
    "        \n",
    "        if(len(weights) > 0):\n",
    "            # weights must be a matrix size: network_size x network_size\n",
    "            self.cns.weights = csr_matrix(weights)\n",
    "        if(len(biases) > 0):\n",
    "            self.cns.biases = biases\n",
    "        if(len(taus) > 0):\n",
    "            self.cns.taus = taus\n",
    "        if(len(gains) > 0):\n",
    "            self.gains = gains\n",
    "    \n",
    "    def act(self, observation, reward, done):\n",
    "        external_inputs = np.asarray([0.0]*self.network_size)\n",
    "        external_inputs[0:self.n_observations] = observation\n",
    "        self.cns.euler_step(external_inputs)\n",
    "        output = 2.0 * (self.cns.outputs[-self.n_actions:] - 0.5)\n",
    "        return output\n",
    "\n",
    "def evaluate(genome, seed = 0):\n",
    "    # create the phenotype from the genotype:\n",
    "    agent = CTRNN_agent(n_neurons, genome=genome)\n",
    "    # run the agent:\n",
    "    reward = run_cart.run_cart_continuous(agent, simulation_seed=seed, graphics=False)\n",
    "    #print('Reward = ' + str(reward))\n",
    "    return reward\n",
    "    \n",
    "    \n",
    "# Parameters CTRNN:\n",
    "network_size = 10\n",
    "genome_size = (network_size+3)*network_size\n",
    "\n",
    "# Evolutionary algorithm:\n",
    "n_individuals = 30\n",
    "n_generations = 30\n",
    "p_mut = 0.05\n",
    "n_best = 3\n",
    "\n",
    "Population = np.random.rand(n_individuals, genome_size)\n",
    "Reward = np.zeros([n_individuals,])\n",
    "max_fitness = np.zeros([n_generations,])\n",
    "mean_fitness = np.zeros([n_generations,])\n",
    "Best = []\n",
    "fitness_best = []\n",
    "for g in range(n_generations):\n",
    "    \n",
    "    # evaluate:\n",
    "    for i in range(n_individuals):\n",
    "        Reward[i] = evaluate(Population[i, :])\n",
    "    mean_fitness[g] = np.mean(Reward)\n",
    "    max_fitness[g] = np.max(Reward)\n",
    "    print('Generation {}, mean = {} max = {}'.format(g, mean_fitness[g], max_fitness[g]))\n",
    "    # select:\n",
    "    inds = np.argsort(Reward)\n",
    "    inds = inds[-n_best:]\n",
    "    if(len(Best) == 0 or Reward[-1] > fitness_best):\n",
    "        Best = Population[inds[-1], :]\n",
    "        fitness_best = Reward[-1]\n",
    "    # vary:\n",
    "    NewPopulation = np.zeros([n_individuals, genome_size])\n",
    "    for i in range(n_individuals):\n",
    "        ind = inds[i % n_best]\n",
    "        NewPopulation[i,:] = Population[ind, :]\n",
    "        for gene in range(genome_size):\n",
    "            if(np.random.rand() <= p_mut):\n",
    "                NewPopulation[i,gene] = np.random.rand()\n",
    "    Population = NewPopulation\n",
    "\n",
    "print('Best fitness ' + str(fitness_best))\n",
    "print('Genome = ' + str(Best))\n",
    "\n",
    "plt.figure();\n",
    "plt.plot(range(n_generations), mean_fitness)\n",
    "plt.plot(range(n_generations), max_fitness)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
